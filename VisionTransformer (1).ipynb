{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra1ph2/Vision-Transformer/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7v2dPqfZFIV"
      },
      "source": [
        "#### Libraries Imported and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bQIxfD9Ep0y"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS7R221iUMI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7db7e5-3a6d-4b14-e988-4a93e6143b9c"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFc5EfTgZIBk"
      },
      "source": [
        "#### Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_75W0devEPY7"
      },
      "source": [
        "##### Vision Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idmPOFlDIGDZ"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention Module used to perform self-attention operation allowing the model to attend\n",
        "    information from different representation subspaces on an input sequence of embeddings.\n",
        "    The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> Query, Key, Value -> ReshapeHeads -> Query.TransposedKey -> Softmax -> Dropout\n",
        "    -> AttentionScores.Value -> ReshapeHeadsBack -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        _reshape_heads(inp) :-\n",
        "        Changes the input sequence embeddings to reduced dimension according to the number\n",
        "        of attention heads to parallelize attention operation\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size * heads, seq_len, reduced_dim)\n",
        "\n",
        "        _reshape_heads_back(inp) :-\n",
        "        Changes the reduced dimension due to parallel attention heads back to the original\n",
        "        embedding size\n",
        "        (batch_size * heads, seq_len, reduced_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        forward(inp) :-\n",
        "        Performs the self-attention operation on the input sequence embedding.\n",
        "        Returns the output of self-attention as well as atttention scores\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim), (batch_size * heads, seq_len, seq_len)\n",
        "\n",
        "    Examples:\n",
        "        >>> attention = Attention(embed_dim, heads, activation, dropout)\n",
        "        >>> out, weights = attention(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, dropout=0.1):\n",
        "        super(Attention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        query = self.activation(self.query(inp))\n",
        "        key   = self.activation(self.key(inp))\n",
        "        value = self.activation(self.value(inp))\n",
        "\n",
        "        # output of _reshape_heads(): (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        query = self._reshape_heads(query)\n",
        "        key   = self._reshape_heads(key)\n",
        "        value = self._reshape_heads(value)\n",
        "\n",
        "        # attention_scores: (batch_size * heads, seq_len, seq_len) | Softmaxed along the last dimension\n",
        "        attention_scores = self.softmax(torch.matmul(query, key.transpose(1, 2)))\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        out = torch.matmul(self.dropout(attention_scores), value)\n",
        "\n",
        "        # output of _reshape_heads_back(): (batch_size, seq_len, embed_size)\n",
        "        out = self._reshape_heads_back(out)\n",
        "\n",
        "        return out, attention_scores\n",
        "\n",
        "    def _reshape_heads(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "\n",
        "        reduced_dim = self.embed_dim // self.heads\n",
        "        assert reduced_dim * self.heads == self.embed_dim\n",
        "        out = inp.reshape(batch_size, seq_len, self.heads, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(-1, seq_len, reduced_dim)\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        return out\n",
        "\n",
        "    def _reshape_heads_back(self, inp):\n",
        "        # inp: (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        batch_size_mul_heads, seq_len, reduced_dim = inp.size()\n",
        "        batch_size = batch_size_mul_heads // self.heads\n",
        "\n",
        "        out = inp.reshape(batch_size, self.heads, seq_len, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYemII1gElcK"
      },
      "source": [
        "# Check if Dropout should be used after second Linear Layer\n",
        "class FeedForward(nn.Module):\n",
        "    '''\n",
        "    FeedForward Network with two sequential linear layers with GELU activation function\n",
        "    ,applied to the output of self attention operation. The sequence of operations is as\n",
        "    follows :-\n",
        "\n",
        "    Input -> FC1 -> GELU -> Dropout -> FC2 -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> FF = FeedForward(8, 1)\n",
        "        >>> out = FF(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, forward_expansion=1, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim * forward_expansion)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim * forward_expansion, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.dropout(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMrCi7DNGvCc"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    '''\n",
        "    Transformer Block combines both the attention module and the feed forward module with layer\n",
        "    normalization, dropout and residual connections. The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> LayerNorm1 -> Attention -> Residual -> LayerNorm2 -> FeedForward -> Output\n",
        "      |                                   |  |                                      |\n",
        "      |-------------Addition--------------|  |---------------Addition---------------|\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> TB = TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout)\n",
        "        >>> out = TB(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = Attention(embed_dim, heads, activation, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForward(embed_dim, forward_expansion, dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        res = inp\n",
        "        out = self.norm1(inp)\n",
        "        out, _ = self.attention(out)\n",
        "        out = out + res\n",
        "\n",
        "        res = out\n",
        "        out = self.norm2(out)\n",
        "        out = self.feed_forward(out)\n",
        "        out = out + res\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcBcReoPJ4NC"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    '''\n",
        "    Transformer combines multiple layers of Transformer Blocks in a sequential manner. The sequence\n",
        "    of the operations is as follows -\n",
        "\n",
        "    Input -> TB1 -> TB2 -> .......... -> TBn (n being the number of layers) -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        layers: Number of Transformer Blocks in the Transformer\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
        "        >>> out = transformer(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, layers, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.trans_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout) for i in range(layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        out = inp\n",
        "        for block in self.trans_blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRN2k5-kLWYG"
      },
      "source": [
        "# Not Exactly Same as Paper\n",
        "class ClassificationHead(nn.Module):\n",
        "    '''\n",
        "    Classification Head attached to the first sequence token which is used as the arbitrary\n",
        "    classification token and used to optimize the transformer model by applying Cross-Entropy\n",
        "    loss. The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> FC1 -> GELU -> Dropout -> FC2 -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        classes: Number of classification classes in the dataset\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, embed_dim) -> (batch_size, classes)\n",
        "\n",
        "    Examples:\n",
        "        >>> CH = ClassificationHead(embed_dim, classes, dropout)\n",
        "        >>> out = CH(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, classes, dropout=0.1):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.classes = classes\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim // 2)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim // 2, classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, embed_dim)\n",
        "        batch_size, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.softmax(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, classes)\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-8nXI7gPxIs"
      },
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    '''\n",
        "    Vision Transformer is the complete end to end model architecture which combines all the above modules\n",
        "    in a sequential manner. The sequence of the operations is as follows -\n",
        "\n",
        "    Input -> CreatePatches -> ClassToken, PatchToEmbed , PositionEmbed -> Transformer -> ClassificationHead -> Output\n",
        "                                   |            | |                |\n",
        "                                   |---Concat---| |----Addition----|\n",
        "\n",
        "    Args:\n",
        "        patch_size: Length of square patch size\n",
        "        max_len: Max length of learnable positional embedding\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        classes: Number of classes in the dataset\n",
        "        layers: Number of Transformer Blocks in the Transformer\n",
        "        channels: Number of channels in the input (Default=3)\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        It outputs the classification output as well as the sequence output of the transformer\n",
        "        (batch_size, channels, width, height) -> (batch_size, classes), (batch_size, seq_len+1, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> ViT = VisionTransformer(atch_size, max_len, embed_dim, classes, layers, channels, heads, activation, forward_expansion, dropout)\n",
        "        >>> class_out, hidden_seq = ViT(inp)\n",
        "    '''\n",
        "    def __init__(self, patch_size, max_len, embed_dim, classes, layers, channels=3, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.name = 'VisionTransformer'\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.channels = channels\n",
        "        self.patch_to_embed = nn.Linear(patch_size * patch_size * channels, embed_dim)\n",
        "        self.position_embed = nn.Parameter(torch.randn((max_len, embed_dim)))\n",
        "        self.transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
        "        self.classification_head = ClassificationHead(embed_dim, classes)\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, channels, width, height)\n",
        "        batch_size, channels, width, height = inp.size()\n",
        "        assert channels == self.channels\n",
        "\n",
        "        out = inp.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size).contiguous()\n",
        "        out = out.view(batch_size, channels, -1, self.patch_size, self.patch_size)\n",
        "        out = out.permute(0, 2, 3, 4, 1)\n",
        "        # out: (batch_size, seq_len, patch_size, patch_size, channels) | seq_len would be (width*height)/(patch_size**2)\n",
        "        batch_size, seq_len, patch_size, _, channels = out.size()\n",
        "\n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.patch_to_embed(out)\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        out = torch.cat([class_token, out], dim=1)\n",
        "        # out: (batch_size, seq_len+1, embed_dim)\n",
        "\n",
        "        position_embed = self.position_embed[:seq_len+1]\n",
        "        position_embed = position_embed.unsqueeze(0).expand(batch_size, seq_len+1, self.embed_dim)\n",
        "        out = out + position_embed\n",
        "        # out: (batch_size, seq_len+1, embed_dim) | Added Positional Embeddings\n",
        "\n",
        "        out = self.transformer(out)\n",
        "        # out: (batch_size, seq_len+1, embed_dim)\n",
        "        class_token = out[:, 0]\n",
        "        # class_token: (batch_size, embed_dim)\n",
        "\n",
        "        class_out = self.classification_head(class_token)\n",
        "        # class_out: (batch_size, classes)\n",
        "\n",
        "        return class_out, out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60fXexWWWlpJ"
      },
      "source": [
        "def CIFAR100DataLoader(split, batch_size=8, num_workers=2, shuffle=True, size='32', normalize='standard'):\n",
        "    '''\n",
        "    A wrapper function that creates a DataLoader for CIFAR100 dataset loaded from torchvision using\n",
        "    the parameters supplied and applies the required data augmentations.\n",
        "\n",
        "    Args:\n",
        "        split: A string to decide if train or test data to be used (Values: 'train', 'test')\n",
        "        batch_size: Batch size to used for loading data (Default=8)\n",
        "        num_workers: Number of parallel workers used to load data (Default=2)\n",
        "        shuffle: Boolean value to decide if data should be randomized (Default=True)\n",
        "        size: A string to decide the size of the input images (Default='32') (Values: '32','224')\n",
        "        normalize: A string to decide the normalization to applied to the input images\n",
        "                   (Default='standard') (Values: 'standard', 'imagenet')\n",
        "\n",
        "    Output:\n",
        "        DataLoader Object\n",
        "    '''\n",
        "    if normalize == 'imagenet':\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    elif normalize == 'standard':\n",
        "        mean = [0.5, 0.5, 0.5]\n",
        "        std =  [0.5, 0.5, 0.5]\n",
        "\n",
        "    if split == 'train':\n",
        "        if size == '224':\n",
        "            train_transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop((224,224), scale=(0.8, 1.0)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        elif size == '32':\n",
        "            train_transform = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(15),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    elif split == 'test':\n",
        "        if size == '224':\n",
        "            test_transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "\n",
        "        elif size == '32':\n",
        "            test_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_YjSG0aLJR"
      },
      "source": [
        "#### Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byfz7zrEaKxq"
      },
      "source": [
        "# Initializations of all the constants used in the training and testing process\n",
        "\n",
        "lr = 0.003\n",
        "batch_size = 256\n",
        "num_workers = 2\n",
        "shuffle = True\n",
        "patch_size = 4\n",
        "image_sz = 32\n",
        "max_len = 100 # All sequences must be less than 1000 including class token\n",
        "embed_dim = 512\n",
        "classes = 10\n",
        "layers = 12\n",
        "channels = 3\n",
        "resnet_features_channels = 64\n",
        "heads = 16\n",
        "epochs = 50"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size = 4\n",
        "max_len = 100\n",
        "embed_dim = 512\n",
        "classes = 100\n",
        "layers = 8\n",
        "channels = 3\n",
        "heads = 16"
      ],
      "metadata": {
        "id": "0VDnJDlMtrJc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d89pJGwZaUBs"
      },
      "source": [
        "def train(model, dataloader, criterion, optimizer, scheduler, resnet_features=None):\n",
        "    '''\n",
        "    Function used to train the model over a single epoch and update it according to the\n",
        "    calculated gradients.\n",
        "\n",
        "    Args:\n",
        "        model: Model supplied to the function\n",
        "        dataloader: DataLoader supplied to the function\n",
        "        criterion: Criterion used to calculate loss\n",
        "        optimizer: Optimizer used update the model\n",
        "        scheduler: Scheduler used to update the learing rate for faster convergence\n",
        "                   (Commented out due to poor results)\n",
        "        resnet_features: Model to get Resnet Features for the hybrid architecture (Default=None)\n",
        "\n",
        "    Output:\n",
        "        running_loss: Training Loss (Float)\n",
        "        running_accuracy: Training Accuracy (Float)\n",
        "    '''\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for data, target in tqdm(dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if model.name == 'VisionTransformer':\n",
        "            with torch.no_grad():\n",
        "                if resnet_features != None:\n",
        "                    data = resnet_features(data)\n",
        "            output, _ = model(data)\n",
        "        elif model.name == 'ResNet':\n",
        "            output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == target).float().mean()\n",
        "        running_accuracy += acc / len(dataloader)\n",
        "        running_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return running_loss, running_accuracy"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXAFKy14EIyU"
      },
      "source": [
        "def evaluation(model, dataloader, criterion, resnet_features=None):\n",
        "    '''\n",
        "    Function used to evaluate the model on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        model: Model supplied to the function\n",
        "        dataloader: DataLoader supplied to the function\n",
        "        criterion: Criterion used to calculate loss\n",
        "        resnet_features: Model to get Resnet Features for the hybrid architecture (Default=None)\n",
        "\n",
        "    Output:\n",
        "        test_loss: Testing Loss (Float)\n",
        "        test_accuracy: Testing Accuracy (Float)\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        test_accuracy = 0.0\n",
        "        test_loss = 0.0\n",
        "        for data, target in tqdm(dataloader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            if model.name == 'VisionTransformer':\n",
        "                if resnet_features != None:\n",
        "                    data = resnet_features(data)\n",
        "                output, _ = model(data)\n",
        "            elif model.name == 'ResNet':\n",
        "                output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc = (output.argmax(dim=1) == target).float().mean()\n",
        "            test_accuracy += acc / len(dataloader)\n",
        "            test_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1NNc-FxvJU7"
      },
      "source": [
        "#### Model Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94FIkGknvMUW"
      },
      "source": [
        "Run either one the following subcells according to the models selected to train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofAkCvMzfN3R"
      },
      "source": [
        "##### Model - Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG_wT7pXe5q9"
      },
      "source": [
        "# Vision Transformer Architecture\n",
        "\n",
        "model = VisionTransformer(\n",
        "    patch_size=patch_size,\n",
        "    max_len=max_len,\n",
        "    embed_dim=embed_dim,\n",
        "    classes=classes,\n",
        "    layers=layers,\n",
        "    channels=channels,\n",
        "    heads=heads).to(device)\n",
        "\n",
        "resnet_features = None"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7yDEtyzw8oU"
      },
      "source": [
        "##### CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1l68Z_UiGc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "3910a2cf-416a-47e6-fa8c-b469f5f3046d"
      },
      "source": [
        "train_dataloader = CIFAR100DataLoader(split='train', batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, size='32', normalize='standard')\n",
        "test_dataloader = CIFAR100DataLoader(split='test', batch_size=batch_size, num_workers=num_workers, shuffle=False, size='32', normalize='standard')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
        "\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss, running_accuracy = train(model, train_dataloader, criterion, optimizer, scheduler, resnet_features)\n",
        "    print(f\"Epoch : {epoch+1} - acc: {running_accuracy:.4f} - loss : {running_loss:.4f}\\n\")\n",
        "    train_accs.append(running_accuracy)\n",
        "\n",
        "    test_loss, test_accuracy = evaluation(model, test_dataloader, criterion, resnet_features)\n",
        "    print(f\"test acc: {test_accuracy:.4f} - test loss : {test_loss:.4f}\\n\")\n",
        "    test_accs.append(test_accuracy)\n",
        "\n",
        "    if (epoch+1)%5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model,\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "            'train_acc': train_accs,\n",
        "            'test_acc': test_accs\n",
        "        }, '/content/drive/MyDrive/' + model.name + '_CIFAR100_checkpoint.pt')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 33/196 [18:11<1:29:49, 33.06s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c3f31cd04e2f>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch : {epoch+1} - acc: {running_accuracy:.4f} - loss : {running_loss:.4f}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2be30bf1523e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, scheduler, resnet_features)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGrryK4S1AE_",
        "outputId": "b898b846-653f-438a-b3b3-05f98deef8f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KjgBoZoxU42"
      },
      "source": [
        "##### Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the checkpoint\n",
        "checkpoint = torch.load('/content/drive/MyDrive/VisionTransformer_CIFAR100_checkpoint.pt', map_location=torch.device('cpu'))\n",
        "\n",
        "# Retrieve the saved components\n",
        "epoch = checkpoint['epoch']\n",
        "model = checkpoint['model']        # The entire model object is saved, so you can retrieve it directly\n",
        "optimizer = checkpoint['optimizer']\n",
        "scheduler = checkpoint['scheduler']\n",
        "train_accs = checkpoint['train_acc']\n",
        "test_accs = checkpoint['test_acc']\n",
        "\n",
        "print(f\"Loaded checkpoint at epoch {epoch}\")\n",
        "print(test_accs)\n",
        "print(train_accs)"
      ],
      "metadata": {
        "id": "5ILTHBluLzsz",
        "outputId": "76e79120-af80-4468-b488-d4f5f079b441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-a90f88ed538a>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('/content/drive/MyDrive/VisionTransformer_CIFAR100_checkpoint.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint at epoch 34\n",
            "[tensor(0.2747), tensor(0.2822), tensor(0.2894), tensor(0.2936), tensor(0.3045), tensor(0.3040), tensor(0.3131), tensor(0.3107), tensor(0.3277), tensor(0.3249), tensor(0.3353), tensor(0.3375), tensor(0.3353), tensor(0.3418), tensor(0.3445), tensor(0.3514), tensor(0.3513), tensor(0.3591), tensor(0.3518), tensor(0.3570), tensor(0.3650), tensor(0.3687), tensor(0.3649), tensor(0.3734), tensor(0.3742), tensor(0.3843), tensor(0.3844), tensor(0.3842), tensor(0.3883), tensor(0.3857), tensor(0.3801), tensor(0.3890), tensor(0.3948), tensor(0.3919), tensor(0.3939)]\n",
            "[tensor(0.2550), tensor(0.2707), tensor(0.2764), tensor(0.2861), tensor(0.2939), tensor(0.3011), tensor(0.3107), tensor(0.3171), tensor(0.3206), tensor(0.3310), tensor(0.3340), tensor(0.3391), tensor(0.3479), tensor(0.3502), tensor(0.3592), tensor(0.3673), tensor(0.3751), tensor(0.3783), tensor(0.3834), tensor(0.3892), tensor(0.3970), tensor(0.4088), tensor(0.4092), tensor(0.4149), tensor(0.4209), tensor(0.4275), tensor(0.4325), tensor(0.4386), tensor(0.4434), tensor(0.4499), tensor(0.4567), tensor(0.4670), tensor(0.4715), tensor(0.4710), tensor(0.4819)]\n"
          ]
        }
      ]
    }
  ]
}